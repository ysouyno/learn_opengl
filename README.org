#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline
#+options: author:t broken-links:nil c:nil creator:nil
#+options: d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:t
#+options: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t
#+options: timestamp:t title:t toc:t todo:t |:t
#+title: learn_opengl
#+date: <2024-09-27 周五>
#+author: ysouyno
#+email:
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 29.4 (Org mode 9.6.15)
#+cite_export:

* <2024-09-29 周日> 前言

教程链接：“[[https://learnopengl-cn.github.io/][LearnOpenGL CN]]”。

* <2021-12-05 周日> 环境配置（ ~linux~ ）

** ~archlinux~ 环境配置

分别用到了 ~glfw~ ， ~stb~ ， ~glm~ 和 ~glad~ ，其中 ~glfw~ 和 ~stb~ 直接从 ~github~ 上克隆最新代码：

#+begin_src shellsession
$ git clone https://github.com/glfw/glfw.git
$ git clone https://github.com/nothings/stb.git
#+end_src

其中 ~glfw~ 需要编译一下才能使用，很简单：

#+begin_src shellsession
$ cd glfw
$ mkdir build
$ cd build
$ cmake ..
$ make
#+end_src

~glm~ 是按照教程中的建议，选择了低于 ~0.9.9~ 的版本，下载地址：“[[https://github.com/g-truc/glm/releases/tag/0.9.8.5][GLM 0.9.8.5]]”。

~glad~ 用的好像是教程中提供的在线服务链接生成的“[[https://glad.dav1d.de][Glad]]”，另项目中的 ~glad.c~ 是指向本机电脑中的一个软链接。

** 编译

编译及链接选项参考项目的 ~.vscode~ 中的文件。另因为使用 ~clang-format~ 格式化代码，头文件的顺序会影响编译，所以我在 ~~/.clang-format~ 中关闭了这项功能。

#+begin_src shellsession
$ cat ~/.clang-format
Language: Cpp
SortIncludes: false
#+end_src

* <2024-09-27 周五> 环境配置（ ~windows~ ）

** ~vs2010~

1. 创建 ~solution~ 文件 ~learn_opengl.sln~ 。
2. 创建 ~third_party~ 目录。
3. 下载预编译好的“[[https://github.com/glfw/glfw/releases/download/3.3.5/glfw-3.3.5.bin.WIN32.zip][glfw-3.3.5.bin.WIN32.zip]]”。

** ~assimp~

想在 ~vs2010~ 上自己编译 ~assimp~ 的，发现 ~cmake~ 自 ~3.25~ 版本已经移除了 ~vs2010~ ，所以我选择了 ~cmake-3.24.4~ 版本。其实不算什么，只是提醒一下以节省时间。

这个库很难编译，或者说非常麻烦。我在这里“[[https://sourceforge.net/projects/assimp/files/assimp-3.1/][assimp-3.1.1-win-binaries.zip]]”找到了一个预编译二进制包下载，但是好像不能运行。居然提示，一个 ~exe~ 的运行依赖另一个 ~exe~ ，开眼了。

#+ATTR_HTML: :width 60%
[[file:files/20240929_0.png]]

目前的进展是，直接双击 ~assimp.exe~ 会提示缺少 ~msvcr110.dll~ 和 ~msvcp110.dll~ ，这两个文件我在网上下载的出现各种问题，最后安装了 ~vcredist_x86.exe~ （即 ~Microsoft Visual C++ 2012 Redistributable (x86)~ ）解决的，虽然 ~assimp.exe~ 成功运行了，但是上面的错误提示框依然存在。所以我只能选择 ~vs2022~ 来编译。因为其实这个库在 ~vs2022~ 上编译非常方便，在 ~cmake-3.24.4~ 上不用任何配置就编译成功了。

#+ATTR_HTML: :width 60%
[[file:files/20240929_1.png]]

我用的是当前最新的版本 ~assimp-5.4.3~ ，运行效果如下：

#+ATTR_HTML: :width 60%
[[file:files/20240929_2.png]]

*** ~assimp-3.1.1~

尝试自己在 ~vs2010~ 上编译 [[https://sourceforge.net/projects/assimp/files/assimp-3.1/][assimp-3.1.1]] ，首先需要安装：

+ ~GoRuntime_DotNetFramework_3.x.exe~
+ ~DXSDK_Jun10.exe~ （ ~win10~ 系统上它依赖于 ~DotNetFramework_3.5~ ）
+ [[https://cmake.org/files/v2.8/cmake-2.8.6-win32-x86.exe][cmake-2.8.6-win32-x86.exe]]

#+ATTR_HTML: :width 60%
[[file:files/20240929_3.png]]

在安装 ~GoRuntime_DotNetFramework_3.x.exe~ 时发生 ~Error Code: S1023~

+ 尝试卸载 ~Microsoft Visual C++ 2010 x86 Redistributable - 1010.0.40219~ ，失败。
+ 尝试卸载 ~Microsoft Visual C++ 2012 Redistributable (x86)~ ，失败。
+ 尝试卸载 ~Microsoft Visual C++ 2010 x64 Redistributable - 1010.0.40219~ ，成功。

这样 ~cmake-2.8.6~ 也运行成功了，附上我的 ~csdn~ 资源：“[[https://download.csdn.net/download/ftuc5dn/89815337][assimp-3.1.1 预编译二进制文件（32位）]]”。

#+ATTR_HTML: :width 60%
[[file:files/20240929_4.png]]

~vs2010~ 成功：

#+ATTR_HTML: :width 60%
[[file:files/20240929_5.png]]

* <2021-12-06 周一> 入门.坐标系统

在“坐标系统”的“更多立方体”中提供的代码，文章中提到这十个立方体都能自主旋转，其实不然。我稍微修改了他的代码实现了，见：“[[https://github.com/ysouyno/learn_opengl/commit/b49247b5f9e1dd5e5b92809fe2235b948185767a][more cubes]]”，主要是这两处修改：

#+begin_src c++
  float angle = 20.0f * (i + 1);
  model = glm::rotate(model, (float)glfwGetTime() * glm::radians(angle),
                      glm::vec3(1.0f, 0.3f, 0.5f));
#+end_src

原代码中 ~angle = 20.0f * i~ 会导致最中间的那个立方体不旋转。

* <2024-09-27 周五> 光照.基础光照

背后的数学知道可能需要回顾，代码中的注释说明了一切：

#+begin_src glsl
  #version 330 core
  out vec4 FragColor;

  in vec3 Normal;
  in vec3 FragPos;

  uniform vec3 objectColor;
  uniform vec3 lightColor;
  uniform vec3 lightPos;

  void main()
  {
    // amibent
    float ambientStrength = 0.1;
    vec3 ambient = ambientStrength * lightColor;

    // diffuse
    // 因为只关心方向向量的方向，所以这里先进行标准化，即 normalize
    // norm 是书中所说的法向量，垂直于立面体表面
    vec3 norm = normalize(Normal);
    vec3 lightDir = normalize(lightPos - FragPos);

    // 将法向量和方向向量进行点乘，会得到它们之前夹角的余弦值
    float diff = max(dot(norm, lightDir), 0.0);
    vec3 diffuse = diff * lightColor;

    vec3 result = (ambient + diffuse) * objectColor;
    FragColor = vec4(result, 1.0);
  }
#+end_src

* <2024-09-27 周五> 光照.材质.设置材质

我按照教程一步一步来操作，但没有得到相同的效果：

#+ATTR_HTML: :width 60%
[[file:files/20240927_0.png]]

#+ATTR_HTML: :width 60%
[[file:files/20240927_1.png]]

原来在设置 ~material.shininess~ 时变量名拼写错误了，看评论说 ~shininess~ 设置为 ~0~ 时会出现黑色。

* <2024-09-28 周六> 光照.光照贴图.练习

练习的最后一题没有提供答案，我的答案是（上图是参考答案，下图是我的答案）：

#+ATTR_HTML: :width 60%
[[file:files/20240928_0.png]]

#+ATTR_HTML: :width 60%
[[file:files/20240928_1.png]]

我是这么修改代码的：

#+ATTR_HTML: :width 60%
[[file:files/20240928_2.png]]

* <2024-09-28 周六> 光照.投光物.平行光

当前 ~commit~ 下可以看到 ~平行光~ 的效果，但是只能显示一个立方体，且不能旋转，与书中提供的贴图效果不一样，且书中提供的代码也与贴图不一致。

看了半天也不知道问题出在哪里，那么在此代码基础上如何显示出十个立方体呢？

这个问题解决了，原来是 ~2.5.light_cube.vs~ 的问题，代码中一直在用 ~aPos~ ，它是直接来自输入参数，并没有 ~model~ 的计算处理，所以只能显示一个立方体，所以现在用 ~FragPos~ 就解决了。

* <2024-09-28 周六> 光照.复习.词汇表

这一部分的内容真的好多，需要时不时的温故一下，否则学习的时间又浪费了！两天的时间居然学习了这么多新知识：

+ 颜色向量（ ~Color Vector~ ）：一个通过红绿蓝（ ~RGB~ ）分量的组合描绘大部分真实颜色的向量。一个物体的颜色实际上是该物体所不能吸收的反射颜色分量。
+ 风氏光照模型（ ~Phong Lighting Model~ ）：一个通过计算环境光，漫反射，和镜面光分量的值来近似真实光照的模型。
+ 环境光照（ ~Ambient Lighting~ ）：通过给每个没有被光照的物体很小的亮度，使其不是完全黑暗的，从而对全局光照进行近似。
+ 漫反射着色（ ~Diffuse Shading~ ）：一个顶点/片段与光线方向越接近，光照会越强。使用了法向量来计算角度。
+ 法向量（ ~Normal Vector~ ）：一个垂直于平面的单位向量。
+ 法线矩阵（ ~Normal Matrix~ ）：一个 ~3x3~ 矩阵，或者说是没有平移的模型（或者模型-观察）矩阵。它也被以某种方式修改（逆转置），从而在应用非统一缩放时，保持法向量朝向正确的方向。否则法向量会在使用非统一缩放时被扭曲。
+ 镜面光照（ ~Specular Lighting~ ）：当观察者视线靠近光源在表面的反射线时会显示的镜面高光。镜面光照是由观察者的方向，光源的方向和设定高光分散量的反光度值三个量共同决定的。
+ 风氏着色（ ~Phong Shading~ ）：风氏光照模型应用在片段着色器。
+ ~Gouraud~ 着色（ ~Gouraud shading~ ）：风氏光照模型应用在顶点着色器上。在使用很少数量的顶点时会产生明显的瑕疵。会得到效率提升但是损失了视觉质量。
+ ~GLSL~ 结构体（ ~GLSL struct~ ）：一个类似于 ~C~ 的结构体，用作着色器变量的容器。大部分时间用来管理输入/输出/ ~uniform~ 。
+ 材质（ ~Material~ ）：一个物体反射的环境光，漫反射，镜面光颜色。这些东西设定了物体所拥有的颜色。
+ 光照属性（ ~Light~ （ ~properties~ ））：一个光的环境光，漫反射，镜面光的强度。可以使用任何颜色值，对每一个风氏分量（ ~Phong Component~ ）定义光源发出的颜色/强度。
+ 漫反射贴图（ ~Diffuse Map~ ）：一个设定了每个片段中漫反射颜色的纹理图片。
+ 镜面光贴图（ ~Specular Map~ ）：一个设定了每一个片段的镜面光强度/颜色的纹理贴图。仅在物体的特定区域显示镜面高光。
+ 定向光（ ~Directional Light~ ）：只有方向的光源。它被建模为无限距离，这使得它的所有光线看起来都是平行的，因此它的方向矢量在整个场景中保持不变。
+ 点光源（ ~Point Light~ ）：一个在场景中有位置的，光线逐渐衰减的光源。
+ 衰减（ ~Attenuation~ ）：光随着距离减少强度减小的过程，通常使用在点光源和聚光下。
+ 聚光（ ~Spotlight~ ）：一个被定义为在某一个方向上的锥形的光源。
+ 手电筒（ ~Flashlight~ ）：一个摆放在观察者视角的聚光。
+ ~GLSL Uniform~ 数组（ ~GLSL Uniform Array~ ）：一个 ~uniform~ 值数组。它的工作原理和 ~C~ 语言数组大致一样，只是不能动态分配内存。

* <2024-10-01 周二> 高级 ~OpenGL~ .模板测试.物体轮廓（一）

注意到这个变量非常重要 ~GL_STENCIL_BUFFER_BIT~ ，如果不设置它，效果非常的差，你可以试着取消它看看效果。

说实话，我真的没有看懂这一节的内容！

* <2024-10-01 周二> 高级 ~OpenGL~ .模板测试.物体轮廓（二）

关于物体轮廓的 ~stencil~ 代码流程理解：

#+begin_src c++
  // configure global opengl state
  // -----------------------------
  glEnable(GL_DEPTH_TEST);
  glDepthFunc(GL_LESS);
  glEnable(GL_STENCIL_TEST);
  glStencilFunc(GL_NOTEQUAL, 1, 0xFF);
  glStencilOp(GL_KEEP, GL_KEEP, GL_REPLACE);
#+end_src

在全局设置中，先启用 ~stencil~ 测试，并设置不为 ~1~ 时通过测试（这里似乎没什么用），这里有两个函数通俗点讲：

+ ~glStencilFunc~ 设置对缓冲区做什么，比如等于 ~1~ 时通过测试。
+ ~glStencilOp~ 设置如何更新缓冲区，比如上述代码，第一个参数表示深度测试通过但 ~stencil~ 测试失败时保留缓冲区，第二个参数表示 ~stencil~ 测试通过但深度测试失败时保留缓冲区，第三个参数表示两个测试都通过时将模板值设置为 ~glStencilFunc~ 函数设置的 ~ref~ 值。

#+begin_src c++
  // draw floor as normal, but don't write the floor to the stencil buffer,
  // we only care about the containers. We set its mask to 0x00 to not write to the stencil buffer.
  glStencilMask(0x00);
#+end_src

先绘制地板，因为地板不进模板测试缓冲区，所以设置 ~mask~ 为 ~0x00~ ，此时的模板测试缓冲区为空。

#+begin_src c++
  // 1st. render pass, draw objects as normal, writing to the stencil buffer
  // --------------------------------------------------------------------
  glStencilFunc(GL_ALWAYS, 1, 0xFF);
  glStencilMask(0xFF);
#+end_src

绘制完地板后要写入模板测试缓冲区了，所以此时将 ~mask~ 设置为 ~0xFF~ ，表示启用模板缓冲区写入。通过使用 ~GL_ALWAYS~ 模板测试函数，我们保证了箱子的每个片段都会将模板缓冲的模板值更新为 ~1~ 。

+ ~glStencilFunc(GL_ALWAYS, 1, 0xFF);~ 表示总是将模板值与参考值 ~1~ 进行比较？（对这个函数好抽象）

紧接着绘制立方体，这样立方体的数据就写入到了模板测试缓冲区里了。此时缓冲区里只有两个立方体的数据（没有地板哦），然后设置：

#+begin_src c++
  // 2nd. render pass: now draw slightly scaled versions of the objects, this time disabling stencil writing.
  // Because the stencil buffer is now filled with several 1s. The parts of the buffer that are 1 are not drawn, thus only drawing
  // the objects' size differences, making it look like borders.
  // -----------------------------------------------------------------------------------------------------------------------------
  glStencilFunc(GL_NOTEQUAL, 1, 0xFF);
  glStencilMask(0x00);
  glDisable(GL_DEPTH_TEST);
  shaderSingleColor.use();
  float scale = 1.1f;
#+end_src

这里设置不为 ~1~ 时通过模板测试，所以已经绘制的两个立方体之外的区域才可以通过模板测试，并禁止了模板缓冲区的写入。同时禁用了深度测试，它的目的是为了在绘制时边框不会被地板覆盖。

这里的伪代码表达了整个流程，非常直观：

#+begin_src c++
  glEnable(GL_DEPTH_TEST);
  glStencilOp(GL_KEEP, GL_KEEP, GL_REPLACE);

  glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT | GL_STENCIL_BUFFER_BIT);

  glStencilMask(0x00); // 记得保证我们在绘制地板的时候不会更新模板缓冲
  normalShader.use();
  DrawFloor()

  glStencilFunc(GL_ALWAYS, 1, 0xFF);
  glStencilMask(0xFF);
  DrawTwoContainers();

  glStencilFunc(GL_NOTEQUAL, 1, 0xFF);
  glStencilMask(0x00);
  glDisable(GL_DEPTH_TEST);
  shaderSingleColor.use();
  DrawTwoScaledUpContainers();
  glStencilMask(0xFF);
  glEnable(GL_DEPTH_TEST);
#+end_src

* <2024-10-09 周三> 高级 ~OpenGL~ .几何着色器

#+begin_src glsl
  void build_house(vec4 position) {
    fColor = gs_in[0].color;
    gl_Position = position + vec4(-0.2, -0.2, 0.0, 0.0);
    EmitVertex();
    gl_Position = position + vec4( 0.2, -0.2, 0.0, 0.0);
    EmitVertex();
    gl_Position = position + vec4(-0.2,  0.2, 0.0, 0.0);
    EmitVertex();
    gl_Position = position + vec4( 0.2,  0.2, 0.0, 0.0);
    EmitVertex();
    gl_Position = position + vec4( 0.0,  0.4, 0.0, 0.0);
    fColor = vec3(1.0, 1.0, 1.0); // 屋顶上的落雪效果
    EmitVertex();
    EndPrimitive();
  }
#+end_src

<<白色屋顶>>
为啥这里加上 ~fColor = vec3(1.0, 1.0, 1.0);~ 的效果是屋顶颜色渐变，以我的理解，不应该将之前 ~fColor = gs_in[0].color;~ 设置的值覆盖掉了嘛，会导致所以四个房子都变成白色的。

我以为在代码中只加载了 ~Model nanosuit("nanosuit/nanosuit.obj");~ 一个文件，那实际上也就只需要一个 ~nanosuit.obj~ 文件即可呢，但其实如果只用这一个文件的话，显示出来的将是全黑色的角色。所以我把缺失的文件都添加上了。

* <2024-10-10 周四> 高级 ~OpenGL~ .实例化.实例化数组

这行 ~glVertexAttribDivisor(2, 1);~ 代码是点睛之笔！如果没有它整个输出是错乱的。我也试着将第二个参数改为 ~2~ ，这样只显示了下半部分；改成 ~5~ 只显示底部的五分之一。

这是实例化数组，上一个 ~commit~ ，使用的是 ~uniform~ 来传递，它在 ~opengl~ 中有数量上限的限制，所以这里采用实例化数组就没有这个限制了，限制它的只有内存了。

绘制顺序将从左下角向右再向上绘制，从该绘制效果上可以看出。

* <2024-10-10 周四> 高级 ~OpenGL~ .实例化.小行星带

哇喔！效果杠杠的。但是这里并没有使用实例化，它用了一个超大的 ~modelMatrices~ 数组来存放小行星的矩阵。最终运行效果：

#+ATTR_HTML: :width 60%
[[file:files/20241010_0.png]]

使用实例化后产生十万个小行星的效果如下，对于办公室这个电脑稍微有点慢了：

#+ATTR_HTML: :width 60%
[[file:files/20241010_1.png]]

我不太理解这段代码：

#+begin_src c++
  // set transformation matrices as an instance vertex attribute (with divisor 1)
  // note: we're cheating a little by taking the, now publicly declared,
  // VAO of the model's mesh(es) and adding new vertexAttribPointers
  // normally you'd want to do this in a more organized fashion, but for learning purposes this will do
  // --------------------------------------------------------------------------------------------------
  for (unsigned int i = 0; i < rock.meshes.size(); ++i) {
    unsigned int VAO = rock.meshes[i].VAO;
    glBindVertexArray(VAO);

    // set attribute pointers for matrix (4 times vec4)
    glEnableVertexAttribArray(3);
    glVertexAttribPointer(3, 4, GL_FLOAT, GL_FALSE, sizeof(glm::mat4), (void*)0);
    glEnableVertexAttribArray(4);
    glVertexAttribPointer(4, 4, GL_FLOAT, GL_FALSE, sizeof(glm::mat4), (void*)(1 * sizeof(glm::vec4)));
    glEnableVertexAttribArray(5);
    glVertexAttribPointer(5, 4, GL_FLOAT, GL_FALSE, sizeof(glm::mat4), (void*)(2 * sizeof(glm::vec4)));
    glEnableVertexAttribArray(6);
    glVertexAttribPointer(6, 4, GL_FLOAT, GL_FALSE, sizeof(glm::mat4), (void*)(3 * sizeof(glm::vec4)));

    glVertexAttribDivisor(3, 1);
    glVertexAttribDivisor(4, 1);
    glVertexAttribDivisor(5, 1);
    glVertexAttribDivisor(6, 1);

    glBindVertexArray(0);
   }
#+end_src

为什么 ~glVertexAttribPointer~ 的第五个参数是 ~sizeof(glm::mat4)~ 而不是 ~sizeof(glm::vec4)~ 呢？

* <2024-10-10 周四> 高级 ~OpenGL~ .抗锯齿

只需要添加两行代码：

#+begin_src c++
  glfwWindowHint(GLFW_SAMPLES, 4);
  glEnable(GL_MULTISAMPLE);
#+end_src

也可以只有一句代码：

#+begin_src c++
  glfwWindowHint(GLFW_SAMPLES, 4);
#+end_src

~glfwWindowHint~ 的第二个参数表示一个像素中有几个采样点，数量越多越平滑。

* <2024-10-10 周四> 高级 ~OpenGL~ .抗锯齿.离屏 ~MSAA~ （一）

关于帧缓冲（ ~framebuffer~ ）的概念对我来说非常容易忘记，这个章节当时国庆假期在家时就反复看，但是从没入心。所以回看帧缓冲这节希望下面的知识点方便理解：

+ 一个完整的帧缓冲需要满足以下的条件：
  - 附加至少一个缓冲（颜色、深度或模板缓冲）
  - 至少有一个颜色附件（ ~Attachment~ ）
  - 所有的附件都必须是完整的（保留了内存）（申请好了足够的内存空间）
  - 每个缓冲都应该有相同的样本数（ ~sample~ ）
+ 渲染到一个不同的帧缓冲被叫做离屏渲染（ ~Off-screen Rendering~ ）。
+ 要保证所有的渲染操作在主窗口中有视觉效果，我们需要再次激活默认帧缓冲，将它绑定到 ~0~ ，即 ~glBindFramebuffer(GL_FRAMEBUFFER, 0);~ 。
+ ~附件~ 是一个内存位置，它能够作为帧缓冲的一个缓冲，可以将它想象为一个图像。当创建一个附件的时候我们有两个选项：
  - 纹理
  - 渲染缓冲对象（ ~Renderbuffer Object~ ）

有 ~颜色~ ， ~深度~ ， ~模板~ 三种附件：

#+begin_quote
由于渲染缓冲对象通常都是只写的，它们会经常用于 ~深度~ 和 ~模板~ 附件。

我们需要深度和模板值用于测试，但不需要对它们进行采样，所以渲染缓冲对象非常适合它们。当我们不需要从这些缓冲中采样的时候，通常都会选择渲染缓冲对象，因为它会更优化一点。

渲染缓冲对象能为你的帧缓冲对象提供一些优化，但知道什么时候使用渲染缓冲对象，什么时候使用纹理是很重要的。通常的规则是，如果你不需要从一个缓冲中采样数据，那么对这个缓冲使用渲染缓冲对象会是明智的选择。如果你需要从缓冲中采样颜色或深度值等数据，那么你应该选择纹理附件。性能方面它不会产生非常大的影响的。
#+end_quote

复习至此为止，最后交待一下为什么有 ~帧缓冲~ ，因为有了它可以方便的进行：
+ 反相处理（ ~Inversion~ ）
+ 灰度处理（ ~Grayscale~ ）
+ 模糊效果（ ~Blur~ ）
+ 边缘检测（ ~Edge-detection~ ）

等等等。

为了达到学习效果，现将在 ~advanced_opengl_anti_aliasing_msaa~ 工程基础上生成的 ~advanced_opengl_anti_aliasing_offscreen~ 工程改成使用帧缓冲的有锯齿的渲染效果。

* <2024-10-11 周五> 高级 ~OpenGL~ .抗锯齿.离屏 ~MSAA~ （二）

应该说我对于 ~帧缓冲~ 的理解更深入了，上节提到的将现在工程中的有锯齿效果采用帧缓冲实现一下，然后在会下一个 ~commit~ 中将有锯齿的帧缓冲变成抗锯齿的帧缓冲。

我是这么修改成帧缓冲的（我是充分研究了 ~advanced_opengl_framebuffers~ 工程的）：

1. 创建帧缓冲的操作不用说，放在循环外面实现（详见上上个 ~commit: 13c20bf5ceb2c5afa4bc79f0176ac91d97533822 (still trying)~ ）
2. 然后在循环内部先将创建的新帧缓冲绑定为当前有效，并启用深度测试，紧接着正常绘制立方体。
   + 这里为什么要启用深度测试？因为在循环结尾时为了显示 ~screen-space quad~ ，将深度测试给禁用了。
3. 然后绑定默认的帧缓冲为当前有效，再禁用深度测试。
4. 最后在循环尾部绘制 ~quad plane~ ，这样，一次循环结束。

最后的帧缓冲的有锯齿绘制效果如下：

#+ATTR_HTML: :width 60%
[[file:files/20241011_0.png]]

* <2024-10-12 周六> 高级 ~OpenGL~ .抗锯齿.离屏 ~MSAA~ （三）

这段代码的用意是啥？书中不曾讲，但是我猜可能跟我上节中实现的帧缓冲的方式不太一样导致：

#+begin_src c++
  // configure second post-processing framebuffer
  unsigned int intermediateFBO;
  glGenFramebuffers(1, &intermediateFBO);
  glBindFramebuffer(GL_FRAMEBUFFER, intermediateFBO);
  // create a color attachment texture
  unsigned int screenTexture;
  glGenTextures(1, &screenTexture);
  glBindTexture(GL_TEXTURE_2D, screenTexture);
  glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, SCR_WIDTH, SCR_HEIGHT, 0, GL_RGB, GL_UNSIGNED_BYTE, NULL);
  glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);
  glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);
  glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, screenTexture, 0); // we only need a color buffer
#+end_src

~glBlitFramebuffer~ 这个函数就像 ~gdi~ 函数 ~BitBlt~ 似的进行了两个 ~DC~ 的拷贝。那 ~intermediateFBO~ 似乎就像它的名字中的 ~intermediate~ 一样，就是一个中间位置，但是它 ~intermediateFBO~ 又是怎么绘制到 ~GL_FRAMEBUFFER~ 的默认位置 ~0~ 处的呢？（ ~glBindFramebuffer(GL_FRAMEBUFFER, 0);~ 绑定到默认帧缓冲 ~0~ 处）

这是 ~离屏 MSAA~ 的最终效果（可以和上图比较一下抗锯齿效果）：

#+ATTR_HTML: :width 60%
[[file:files/20241012_0.png]]

补充一下上一小段中提到的这个 ~intermediateFBO~ ，其实它是本节内容中最后部分中描述的要在帧缓冲中实现后期制作效果引入的内容，但是代码我却抄早了。要不然怎么设置应该显示为绿色的立方体显示成了灰色呢！

* <2024-10-12 周六> 高级光照.高级光照. ~Blinn-Phong~

按 ~B~ 键来回切换 ~Phong~ 和 ~Blinn-Phong~ 过程中有两块黑色区域一闪而过，不知道这正常不？

#+ATTR_HTML: :width 60%
[[file:files/20241012_1.gif]]

* <2024-10-12 周六> 高级光照. ~Gamma~ 校正

盗个图，来讲讲我理解的 ~gamma~ 校正

#+ATTR_HTML: :width 40%
[[file:files/gamma_correction_gamma_curves.png]]

+ 颜色用浮点数表示在 ~0.0-1.0~ 之间。
  - 颜色的 ~1/2.2~ 次方（幂）是凸形的暗红色虚线。
  - 颜色的 ~2.2~ 次方（幂）是凹形的暗红色实线。
  - 直虚线表示人眼看到的真实的物理世界。
+ 由于显示器的物理属性偏暗，即小于 ~1~ 的数的 ~2.2~ 次方（幂）的值比 ~1~ 小。
  - 如果没有 ~gamma~ 校正，期望显示效果是直虚线，而实际显示器显示出来的却是暗红色实线，偏暗。
  - 如果有了 ~gamma~ 校正，校正后的是暗红色虚线，这样经过显示器显示后的就是直虚线，显示正常。
+ 所以 ~gamma~ 校正就是将颜色变成 ~1/2.2~ （即 ~0.45~ ）次方（幂），将其变亮。

对 ~sRGB~ 目前没有概念，只知道 ~sRGB~ 空间中存放的是 ~gamma~ 校正后的颜色，这样就引出 ~OpenGL~ 中的 ~sRGB纹理~ 的概念：即当你面对着显示器制作纹理时，这个纹理肯定是在 ~sRGB~ 空间中的，这样很直观，这样制作并保存的纹理就是 ~gamma~ 校正过的，当这个纹理提供给别人使用时，可能再被 ~gamma~ 校正一次，这样就变成了两次 ~gamma~ 校正了，这样效果更亮，就不是所期望的效果了，所以需要重校， ~OpenGL~ 提供了方案来解决这个麻烦，这就是 ~GL_SRGB~ 和 ~GL_SRGB_ALPHA~ 内部纹理格式。

* <2024-10-12 周六> 高级光照.阴影映射（一）

正式开始学习 ~阴影映射~ ，基于深度测试的代码进行修改，换成了 ~wood.png~ 的皮肤，所以初始状态是这样的：

#+ATTR_HTML: :width 60%
[[file:files/20241012_2.png]]

* <2024-10-13 周日> 复习（一）

还需要从头再回看一次，由于进度太快，前面学得忘了，这里阴影映射部分已经进行不下去了！

** 第一章，《你好，三角形》

#+begin_quote
标准化设备坐标（ ~Normalized Device Coordinates, NDC~ ）

一旦你的顶点坐标已经在顶点着色器中处理过，它们就应该是 *标准化设备坐标* 了。
#+end_quote

这就是 ~NDC~ 坐标系，因为 ~opengl~ 的坐标范围是 ~-1.0~ 至 ~1.0~ ：

#+ATTR_HTML: :width 60%
[[file:files/ndc.png]]

#+begin_quote
线框模式（ ~Wireframe Mode~ ）

要想用线框模式绘制你的三角形，你可以通过 ~glPolygonMode(GL_FRONT_AND_BACK, GL_LINE)~ 函数配置 ~OpenGL~ 如何绘制图元。第一个参数表示我们打算将其应用到所有的三角形的正面和背面，第二个参数告诉我们用线来绘制。之后的绘制调用会一直以线框模式绘制三角形，直到我们用 ~glPolygonMode(GL_FRONT_AND_BACK, GL_FILL)~ 将其设置回默认模式。
#+end_quote

之所以这里要引用过来，是因为在几何着色器那节，我只绘制出四个实体颜色小房子，却没有像书中那样绘制效果其中有两个小房子是线框而没有颜色。

** 第一章，《着色器》

#+begin_quote
片段插值（ ~Fragment Interpolation~ ），当渲染一个三角形时，光栅化（ ~Rasterization~ ）阶段通常会造成比原指定顶点更多的片段。光栅会根据每个片段在三角形形状上所处相对位置决定这些片段的位置。基于这些位置，它会插值（ ~Interpolate~ ）所有片段着色器的输入变量。
#+end_quote

因为我隐约感觉在哪里看过，所以这次复习是有意义的，这里就回答了我在[[白色屋顶][白色屋顶]]中的疑问。因为同样的道理你想啊，你只提供了三个顶点，为什么能绘制出一个三角形，在三角形边上的任意一点坐标你也没有提供，但 ~opengl~ 就是绘制出来了，所以虽然你只提供了三个顶点的颜色值，最后却是生成了一个大调色板。

关于 ~uniform~ ，在代码中设置该类型变量，先通过 ~glGetUniformLocation~ 获得 *位置值* ，然后用 ~glUniform~ 设置该 *位置值* 的变量的值。

** 第一章，《纹理》

这节中的《纹理单元》小节，没学扎实，居然没有理解 ~纹理单元~ 的概念，也没有注意到片断着色器中的 ~sampler2D~ 的 ~uniform~ 变量居然在代码中没有为它设置值，那么程序又是如何正确显示纹理的呢？

前提要清楚最终绘制纹理的过程应该是：

_顶点着色器中的是纹理坐标，但是没有纹理像素值，所以片断着色器必须要能访问纹理像素值。_

1. 先激活纹理单元，就是 ~uniform~ 的 *位置值* 。
2. 再绑定纹理单元给采样器。
3. 最后调用 ~glDrawElements~ 绘制出纹理。

即：

#+begin_src c++
  glActiveTexture(GL_TEXTURE0); // 在绑定纹理之前先激活纹理单元
  glBindTexture(GL_TEXTURE_2D, texture);
#+end_src

先 *回想上节讲的 ~uniform~ 变量的位置值* ，再看书中所说的：

#+begin_quote
你可能会奇怪为什么 ~sampler2D~ 变量是个 ~uniform~ ，我们却不用 ~glUniform~ 给它赋值。使用 ~glUniform1i~ ，我们可以给纹理采样器分配一个位置值，这样的话我们能够在一个片段着色器中设置多个纹理。一个 *纹理的位置值* 通常称为一个 *纹理单元（ ~Texture Unit~ ）* 。一个纹理的默认纹理单元是 ~0~ ，它是默认的激活纹理单元，所以教程前面部分我们没有分配一个位置值。
#+end_quote

那么现在第一步满足了，因为本例中只使用了一个纹理，默认的纹理单元 ~0~ 又是默认激活的（纹理单元 ~GL_TEXTURE0~ 默认总是被激活），再看本例代码中：

#+begin_src c++
  // bind Texture
  glBindTexture(GL_TEXTURE_2D, texture);
#+end_src

那么现在第二步也满足了，将纹理单元 ~0~ 绑定给采样器 ~sampler2D~ 了，最后：

#+begin_src c++
  // render container
  ourShader.use();
  glBindVertexArray(VAO);
  glDrawElements(GL_TRIANGLES, 6, GL_UNSIGNED_INT, 0);
#+end_src

这时三个条件都满足了，所以本例代码中的纹理虽然没有调用 ~glUniform~ 给采样器 ~sampler2D~ 赋值，但是却正确显示出来了。

** 第一章，《变换》

这里全是数学知识，向量的，矩阵的，其中有几个图特别得重要，在此盗一下：

1. 缩放：

#+ATTR_HTML: :width 30%
[[file:files/20241013_0.png]]

<<方向向量>>
2. 位移：

#+ATTR_HTML: :width 30%
[[file:files/20241013_1.png]]

*注意如果乘数那个向量的齐次坐标 ~1~ 改为 ~0~ 的话，那么乘法结果都加的是 ~0~ ，导致位移失效，所以齐次坐标为 ~0~ 的坐标就是方向向量（ ~Direction Vector~ ）。*

3. 先缩放 ~2~ 倍，然后位移 ~(1, 2, 3)~ 个单位：

#+ATTR_HTML: :width 30%
[[file:files/20241013_2.png]]

最后一个例子注意（虽然我没亲手尝试）：

#+begin_src c++
  glm::mat4 trans;
  trans = glm::translate(trans, glm::vec3(0.5f, -0.5f, 0.0f));
  trans = glm::rotate(trans, (float)glfwGetTime(), glm::vec3(0.0f, 0.0f, 1.0f));
#+end_src

#+begin_quote
记住，实际的变换顺序应该与阅读顺序相反：尽管在代码中我们先位移再旋转，实际的变换却是先应用旋转再是位移的。
#+end_quote

** 第一章，《坐标系统》

这里的概念很重要，它明确了顶点着色器输出的数据是啥样的：

#+begin_quote
~OpenGL~ 希望在每次顶点着色器运行后，我们可见的所有顶点都为标准化设备坐标（ ~Normalized Device Coordinate, NDC~ ）。也就是说，每个顶点的 ~x~ ， ~y~ ， ~z~ 坐标都应该在 ~-1.0~ 到 ~1.0~ 之间，超出这个坐标范围的顶点都将不可见。我们通常会自己设定一个坐标的范围，之后再在顶点着色器中将这些坐标变换为标准化设备坐标。然后将这些标准化设备坐标传入光栅器（ ~Rasterizer~ ），将它们变换为屏幕上的二维坐标或像素。
#+end_quote

三个矩阵分别是什么用途？

1. 局部空间（顶点数据）至世界空间通过模型矩阵（ ~Model Matrix~ ）
2. 将世界坐标变换到观察空间通过观察矩阵（ ~View Matrix~ ）
3. 为了将顶点坐标从观察变换到裁剪空间，需要定义一个投影矩阵（ ~Projection Matrix~ ）

原来， *透视除法是在每一个顶点着色器运行的最后被自动执行的* ，见：

#+begin_quote
透视除法（ ~Perspective Division~ ）将会执行，在这个过程中我们将位置向量的 ~x~ ， ~y~ ， ~z~ 分量分别除以向量的齐次 ~w~ 分量；透视除法是将 ~4D~ 裁剪空间坐标变换为 ~3D~ 标准化设备坐标的过程。 *这一步会在每一个顶点着色器运行的最后被自动执行。*
#+end_quote

这里也很重要：

#+begin_quote
顶点着色器的输出要求所有的顶点都在裁剪空间内，这正是我们刚才使用变换矩阵所做的。 ~OpenGL~ 然后对 *裁剪坐标* 执行 *透视除法* 从而将它们变换到 *标准化设备坐标* 。 ~OpenGL~ 会使用 ~glViewPort~ 内部的参数来将标准化设备坐标映射到 *屏幕坐标* ，每个坐标都关联了一个屏幕上的点（在我们的例子中是一个 ~800x600~ 的屏幕）。这个过程称为视口变换。
#+end_quote

因此我整理的整个坐标变换流程如下：

1. *局部坐标* 通过 *模型矩阵* 转化为 *世界坐标* 。
2. *世界坐标* 通过 *观察矩阵* 转化为 *观察坐标* 。
3. *观察坐标* 通过 *投影矩阵* 转化为 *裁剪坐标* 。
4. *裁剪坐标* 通过 *透视除法* 转化为 *标准化设备坐标* 。
5. *标准化设备坐标* 使用 ~glViewPort~ 内部的参数转化为 *屏幕坐标* 。

关于模型矩阵书中那个例子值得提一下：

#+begin_quote
你可以将它（模型矩阵）想像为变换一个房子，你需要先将它缩小（它在局部空间中太大了），并将其位移至郊区的一个小镇，然后在 ~y~ 轴上往左旋转一点以搭配附近的房子。
#+end_quote

右手坐标系：

#+ATTR_HTML: :width 30%
[[file:files/coordinate_systems_right_handed.png]]

** 第一章，《摄像机》

这次回来再看 ~摄像机方向~ 就理解了，即：对于摄像机（观察空间）来说，需要四个变量：

1. 摄像机所在的位置
2. 摄像机观察的方向，即 ~z~ 轴负方向
3. 一个指向它右侧的向量，即 ~x~ 轴正方向
4. 一个指向它上方的向量，即 ~y~ 轴正方向

关于摄影机观察的方向，引用：

#+begin_quote
现在我们让摄像机指向场景原点： ~(0, 0, 0)~ 。还记得如果将两个矢量相减，我们就能得到这两个矢量的差吗？用场景原点向量减去摄像机位置向量的结果就是摄像机的指向向量。由于我们知道摄像机指向 ~z~ 轴负方向，但我们希望方向向量（ ~Direction Vector~ ）指向摄像机的 ~z~ 轴正方向。
#+end_quote

#+begin_src c++
  glm::vec3 cameraTarget = glm::vec3(0.0f, 0.0f, 0.0f);
  glm::vec3 cameraDirection = glm::normalize(cameraPos - cameraTarget);
#+end_src

从上面引用的话理解就是：

1. 拿原点 ~cameraTarget~ 减去摄像机位置 ~cameraPos~ 得到的方向向量是，即从摄像机指向原点，也就是局部坐标系的 ~z~ 负方向。
2. 因为希望在摄像机的坐标系中跟局部坐标系保持一致，也是屏幕指向我的方向是正方向，所以上面代码中使用了 ~()cameraPos - cameraTarget)~ 来反射的 ~z~ 的方向。

即摄像机的坐标系最终成为下图的最右侧的样子：

#+ATTR_HTML: :width 60%
[[file:files/camera_axes.png]]

理解了上面的四个变量，这个 ~LookAt矩阵~ 就非常好理解了：

#+begin_src c++
  glm::mat4 view;
  view = glm::lookAt(glm::vec3(0.0f, 0.0f, 3.0f),
                     glm::vec3(0.0f, 0.0f, 0.0f),
                     glm::vec3(0.0f, 1.0f, 0.0f));
#+end_src

1. 三个参数分别表示：
   + 摄像机的位置向量
   + 被观察的目标向量
   + 摄像机的向上向量
2. 有了这三个参数，就能计算出一个摄像机的坐标系，通过叉乘。

* <2024-10-14 周一> 高级光照.阴影映射（二）

#+begin_src c++
  glDrawBuffer(GL_NONE);
  glReadBuffer(GL_NONE);
#+end_src

文中这两行代码表示：
#+begin_quote
告诉 ~OpenGL~ 我们不适用任何颜色数据进行渲染。我们通过将调用 ~glDrawBuffer~ 和 ~glReadBuffer~ 把读和绘制缓冲设置为 ~GL_NONE~ 来做这件事。
#+end_quote

因为没有颜色缓冲对于帧缓冲是不完整的，在后面进行完整性测试时会失败，所以这里是必需的。

#+begin_src c++
  if (glCheckFramebufferStatus(GL_FRAMEBUFFER) != GL_FRAMEBUFFER_COMPLETE) {
    std::cout << "ERROR::FRAMEBUFFER:: Framebuffer is not complete!" << std::endl;
   }
#+end_src

** 深度贴图

关于深度贴图（ ~Depth Map~ ）流程如下：

1. 创建一个帧缓冲对象，创建一个 ~2D~ 深度纹理，并把生成的深度纹理作为帧缓冲的深度缓冲。
2. 首先渲染至深度贴图。
   + 这里就需要以光源的视角了，所以就需要在顶点着色器中将顶点变换到光源。
   + 这里使用了 ~正交投影矩阵~ 和 ~lookat矩阵~ 。
3. 最后像往常一样渲染场景，但这次使用的是深度贴图。

我得到了和书中相同的深度贴图：

#+ATTR_HTML: :width 60%
[[file:files/20241014_0.png]]

备注：顶点着色器和片断着色器的文件名从 ~5.2~ 改为 ~5.3~ ，因为 ~5.2~ 节讲的是 ~gamma~ 校正。

** 渲染阴影

关于渲染阴影的步骤（在生成深度贴图的基础上）：

1. 在顶点着色器中要输出两个变量，后一个变量用于片断着色器中计算阴影。
   1) ~FragPos~ 变量，它表示普通的经变换的世界空间顶点位置（在 ~blinn phong~ 已经见过，不是新内容）。
   2) ~FragPosLightSpace~ 变量，它表示普通世界空间顶点变换后的光空间中顶点位置。
2. 在片断着色器中计算阴影。
   1) 对 ~FragPosLightSpace~ 做透视除法，因为它不是通过 ~gl_Position~ 传入的，所以要自己做。
   2) 计算得的分量在 ~[-1, 1]~ 之间，转化为 ~[0, 1]~ 之间以匹配深度贴图中的深度值。
   3) 计算光的位置视野下最近的深度：
      + ~float closestDepth = texture(shadowMap, projCoords.xy).r;~
   4) 获得片断当前深度，与最近深度一比较即可得到是否在阴影下。

问题： ~texture(shadowMap, projCoords.xy).r~ 不记得书中有交待过 ~texture(...).r~ 可以获得深度值呀！

最终代码效果如下：

#+ATTR_HTML: :width 60%
[[file:files/20241014_1.png]]

** 阴影失真（ ~Shadow Acne~ ）

阴影失真理解起来不太形象，我好奇成因当初是怎么分析出来的？因为 *地板以下* 采样的会出现黑色，所以通过：

#+begin_quote
一个叫做阴影偏移（ ~shadow bias~ ）的技巧来解决这个问题，我们简单的对表面的深度（或深度贴图）应用一个偏移量，这样片段就不会被错误地认为在表面之下了。
#+end_quote

#+ATTR_HTML: :width 60%
[[file:files/shadow_mapping_acne_diagram.png]]

#+ATTR_HTML: :width 60%
[[file:files/shadow_mapping_acne_bias.png]]

** PCF（ ~percentage-closer filtering~ ）

#+begin_quote
PCF（ ~percentage-closer filtering~ ），这是一种多个不同过滤方式的组合，它产生柔和阴影，使它们出现更少的锯齿块和硬边。核心思想是从深度贴图中多次采样，每一次采样的纹理坐标都稍有不同。每个独立的样本可能在也可能不再阴影中。所有的次生结果接着结合在一起，进行平均化，我们就得到了柔和阴影。
#+end_quote

感觉东西越学越多，又出来一个 ~textureSize~ 的 ~opengl~ 函数，它返回一个给定采样器纹理的 ~0~ 级 ~mipmap~ 的 ~vec2~ 类型的宽和高。（这是啥意思？）

回忆：书中讲到的 ~mipmap~ 是在《纹理》那一节中：

#+begin_quote
~OpenGL~ 使用一种叫做多级渐远纹理（ ~Mipmap~ ）的概念来解决这个问题，它简单来说就是一系列的纹理图像，后一个纹理图像是前一个的二分之一。
#+end_quote

这里 ~PCF~ 的意思是通过 ~mipmap~ 搞个偏移量，确保每个新样本来自不同的深度值。

* <2024-10-15 周二> 高级光照.阴影映射（三）

** 点光源阴影（也叫万向阴影贴图 ~omnidirectional shadow maps~ ）

他也分为两个渲染阶段：

1. 首先我们生成深度贴图。
   1) 深度贴图使用的是立方体贴图，它是在 *几何* 着色器中生成的。
   2) 渲染深度立方体贴图时： *几何* 着色器是负责将所有世界空间的顶点变换到 ~6~ 个不同的光空间的着色器。因此 *顶点* 着色器简单地将顶点变换到世界空间（即只乘以 ~模型矩阵~ ），然后直接发送到 *几何* 着色器。
   3) 几何着色器中处理投影和视图矩阵，即将顶点变换到光空间中。
   4) 几何着色器中处理的为什么是 ~6~ 个三角形，共 ~18~ 个点？我的理解是 ~24~ 个呀。
   5) 片段着色器中自己计算深度值（而上节用的是空的片段着色器，是让 ~opengl~ 配置深度贴图的深度值）。
2. 然后使用深度贴图渲染，在场景中创建阴影。

#+begin_src c++
  glm::lookAt(lightPos, lightPos + glm::vec3(1.0f, 0.0f, 0.0f), glm::vec3(0.0f, -1.0f, 0.0f))
#+end_src

这句代码的意思是：

+ 第二个参数表示目标位置是 ~lightPos + glm::vec3(1.0f, 0.0f, 0.0f)~ 即向右看，因为第一个分量是 ~1.0~ 。
+ 第三个参数表示向上的向量 ~glm::vec3(0.0f, -1.0f, 0.0f)~ 是说明 ~y~ 轴的负方向（为什么不是 ~1.0~ 正方向？）

有几个要尝试的地方：

1. 更换顶点顺序看面剔除表现。
2. 上小段中 ~y~ 轴改为正方向的绘制效果。
   1) 修改成 ~1.0~ 后绘制效果是错误的。
   2) 终于想通为什么是 ~-1.0~ 了：
      + ~lightPos~ 虽然是光源位置，这里就是理解中的摄像机位置，因为光源不可能是被观察对象，要不然怎么观察阴影效果呢？所以光源才是摄像机的位置。
      + 那么被观察对象就是 ~lightPos + glm::vec3(1.0f, 0.0f, 0.0f)~ ，即向右的方向。
      + 想像自己正在向右看，观察方向是从自己指向屏幕，中指对应 ~z~ 轴正方向，指向屏幕，大拇指表示 ~x~ 轴指向右，则食指朝下，所以第三个参数就是 ~glm::vec3(0.0f, -1.0f, 0.0f)~ 。
      + 而在《摄像机》那节，从提供的图可以看出，摄像机坐标系的 ~z~ 轴正向是从屏幕指向自己的，所以这里的情况与它是相反的。
3. 我对 ~ShadowCalculation~ 函数中向量减法顺序的疑问。
   1) 经尝试也不能颠倒他们的顺序，绘制效果是错误的。
   2) *但是目前尚无法理解* 。

* <2024-10-16 周三> 复习（二）

再复习一下第二章吧，比如 ~Normal = mat3(transpose(inverse(model))) * aNormal;~ 这句代码你还知道它的用意吗？

** 第二章，《基础光照》

关于 ~最后一件事~ 小节中的 ~法向量~ 提到的，我认为比较重要：

背景： _要知道在片段着色器目前的所有计算（已计算了环境光照 ~ambient~ 和漫反射光照 ~diffuse~ ），都是用的顶点的 *世界坐标* 即常用的变量名 ~FragPos~ 。所以法向量应该也要转化为到世界空间坐标，即对法向量乘以 *模型矩阵* ，但是法向量又不能直接乘以模型矩阵，所以这一节就是在讲这个问题。_

为什么法向量不能直接乘以模型矩阵？因为法向量没有 ~w~ 分量，它是一个方向向量，不支持位移，见我上面的复习内容：[[方向向量][位移]]，引用原文：

#+begin_quote
法向量只是一个方向向量，不能表达空间中的特定位置。同时，法向量没有齐次坐标（顶点位置中的 ~w~ 分量）。这意味着，位移不应该影响到法向量。因此，如果我们打算把法向量乘以一个模型矩阵，我们就要从矩阵中移除位移部分，只选用模型矩阵左上角 ~3×3~ 的矩阵（注意，我们也可以把法向量的 ~w~ 分量设置为 ~0~ ，再乘以 ~4×4~ 矩阵；这同样可以移除位移）。对于法向量，我们只希望对它实施缩放和旋转变换。
#+end_quote

我学到的空间中共有三个操作，正确顺序应该是先旋转，再缩放，最后位移。这三个操作中法向量不支持位移，只能进行前两个操作的计算，然而不等比缩放又会让法向量不垂直于三角形平面而带来问题：

1. 缩放带来的法向量长度的变化可以通过标准化来修复。
2. 垂直问题使用一个为法向量专门定制的模型矩阵。这个矩阵称之为法线矩阵（ ~Normal Matrix~ ）来修复。

引用原文：

#+begin_quote
法线矩阵被定义为「模型矩阵左上角3x3部分的逆矩阵的转置矩阵」。真是拗口，如果你不明白这是什么意思，别担心，我们还没有讨论逆矩阵（ ~Inverse Matrix~ ）和转置矩阵（ ~Transpose Matrix~ ）。注意，大部分的资源都会将法线矩阵定义为应用到模型-观察矩阵（ ~Model-view Matrix~ ）上的操作，但是由于我们只在世界空间中进行操作（不是在观察空间），我们只使用模型矩阵。

在顶点着色器中，我们可以使用 ~inverse~ 和 ~transpose~ 函数自己生成这个法线矩阵，这两个函数对所有类型矩阵都有效。注意我们还要把被处理过的矩阵强制转换为 ~3×3~ 矩阵，来保证它失去了位移属性以及能够乘以 ~vec3~ 的法向量。
#+end_quote

#+begin_src c++
  Normal = mat3(transpose(inverse(model))) * aNormal;
#+end_src

矩阵求逆是一项对于着色器开销很大的运算，你最好先在 ~CPU~ 上计算出法线矩阵，再通过 ~uniform~ 把它传递给着色器（就像模型矩阵一样）。

有一个概念要纠正一下，因为我把 ~漫反射光照~ 和 ~镜面光照~ 理解成一个东西了，我以为都是反射，其实：

1. ~漫反射光照~ 没有反射，直观的理解是光照到物体表面，物体表面上的片段和光值进行计算。因为光源跟一个物体表面的不同片段的法向量的夹角不同，会导致漫反射光照看起来的效果不一样。
   + 如果是平行光的话，漫反射的效果应该是同一表面没有色差。
   + 如果是点光源的话，漫反射的效果应该和反射效果类似。
2. ~镜面光照~ 是真有反射，它通过观察向量与反射光向量的夹角来计算，是我真正意义上理解的反射。

** 第二章，《光照贴图》

这句话也是证实了我上面对 ~漫反射光照~ 和 ~镜面光照~ 的总结：

#+begin_quote
我们也移除了环境光材质颜色向量，因为环境光颜色在几乎所有情况下都等于漫反射颜色。
#+end_quote

可以自己动手制作镜面光纹理贴图：

#+begin_quote
使用 ~Photoshop~ 或 ~Gimp~ 之类的工具，将漫反射纹理转换为镜面光纹理还是比较容易的，只需要剪切掉一些部分，将图像转换为黑白的，并增加亮度/对比度就好了。
#+end_quote

第二章复习至此为止，只复习到《光照贴图》处，后面两节《投光物》和《多光源》似乎对我理解高级光照中的《法线贴图》内容不太关联，所以暂时不复习先。

* <2024-10-16 周三> 高级光照.法线贴图（一）

书中分为两个阶段实现：

1. 第一个阶段：法线贴图
2. 第二个阶段：切线空间

但是书中只有一份代码，是在法线贴图代码的基础上实现的切线空间，所以我把法线贴图的代码给搞出来了，说明我今天做的对第二章的复习还是有成效。我的代码即 ~advanced_lighting_normal_mapping~ 工程，效果如下：

#+ATTR_HTML: :width 60%
[[file:files/20241016_0.png]]

暂时不着急进入切线空间的内容，将书中提到的法线贴图关于法线指向的问题搞懂一下，目前还不明白作者在表达什么！

* <2024-10-17 周四> 高级光照.法线贴图（二）

+这节对我来说太难，公式看不懂，代码看不懂，早上花了一个多小时，依然一头雾水。先放一放吧，代码也等后面看懂了再写+ 。硬上吧！否则接下来的教程《视差贴图》学不了。（评论中有不少人吐槽并举例了翻译的问题，导致翻译过来的不知道在讲什么，看了英文原文才懂。看来我也得看英文原文了）

目前算是看懂了，先了解一下 ~TBN~ 矩阵的用处： _它将法线贴图中的法线坐标左乘 ~TBN~ 矩阵，转换到世界坐标空间中。_ ，什么意思呢？

1. 现在有两个法线向量，一个是原图中的法线向量，一个是贴图中的法线向量，当原图在世界空间中翻转时，它的法线向量是跟着原图的平面在随时变化的，即，始终垂直于原图平面；而贴图中的法线向量是始终指向 ~z~ 方向的（因为这个贴图是蓝色的）。
2. 先用原图中的法线向量（教程中说的法线，也不知道指哪个法线，整得人很难理解），通过给出的切线空间公式计算出 ~TBN~ 矩阵。
3. 有了 ~TBN~ 矩阵，就可以把切线坐标空间的向量转换到世界坐标空间。因此我们把它传给片段着色器中，把通过采样得到的法线坐标左乘上 ~TBN~ 矩阵，转换到世界坐标空间中，这样所有法线和其他光照变量就在同一个坐标系中了。

*我们从法线贴图采样得来的法线向量，是在切线空间表示的，尽管其他光照向量都是在世界空间表示的。* 这句话我是这么理解的：切线空间相对于世界空间来说是一个局部空间，该切线坐标系自己再怎么翻转，它的 ~(0, 0, 1)~ 始终还是指向了切线空间的 ~z~ 正方向，因为法线贴图是蓝色，所以采样法线贴图得到的始终是 ~z~ 正方向， *所以采样得到的法线向量是在切线空间中的* 。

书中提供了两种方法：

1. 第一种方法：将切线坐标变换到世界空间中。
2. 第二种方法：将世界坐标变换到切线空间中。

** 切线坐标变换到世界空间中

这是第一种方法的最终效果：

#+ATTR_HTML: :width 60%
[[file:files/20241017_0.gif]]

代码就不加入 ~git~ 中了，只记录一下如何修改的：

+ 函数 ~renderQuad()~ 中的代码就是按公式计算 ~TBN~ 的坐标并传给顶点着色器（第二种方法也要这么做）
+ 在顶点着色器中将 ~TBN~ 经过 ~model~ 变换到世界空间中，再传给片段着色器。

#+begin_src glsl
  # version 330 core

  layout (location = 0) in vec3 aPos;
  layout (location = 1) in vec3 aNormal;
  layout (location = 2) in vec2 aTexCoords;
  layout (location = 3) in vec3 aTangent;
  layout (location = 4) in vec3 aBitangent;

  out VS_OUT {
    vec3 FragPos;
    vec2 TexCoords;
    mat3 TBN;
  } vs_out;

  uniform mat4 projection;
  uniform mat4 view;
  uniform mat4 model;
  uniform vec3 lightPos;
  uniform vec3 viewPos;

  void main() {
    vs_out.FragPos = vec3(model * vec4(aPos, 1.0));
    vs_out.TexCoords = aTexCoords;
    vec3 T = normalize(vec3(model * vec4(aTangent, 0.0)));
    vec3 B = normalize(vec3(model * vec4(aBitangent, 0.0)));
    vec3 N = normalize(vec3(model * vec4(aNormal, 0.0)));
    vs_out.TBN = mat3(T, B, N);

    gl_Position = projection * view * model * vec4(aPos, 1.0);
  }
#+end_src

+ 在片断着色器中将采样到的 ~normal~ 通过 ~TBN~ 变换到世界坐标中运算。

#+begin_src glsl
  # version 330 core

  out vec4 FragColor;

  in VS_OUT {
    vec3 FragPos;
    vec2 TexCoords;
    mat3 TBN;
  } fs_in;

  uniform sampler2D diffuseMap;
  uniform sampler2D normalMap;
  uniform vec3 lightPos;
  uniform vec3 viewPos;

  void main() {
    // obtain normal from normal map in range [0, 1]
    vec3 normal = texture(normalMap, fs_in.TexCoords).rgb;
    // transform normal vector to range [-1, 1];
    normal = normalize(normal * 2.0 - 1.0);
    normal = normalize(fs_in.TBN * normal);

    // get diffuse color
    vec3 color = texture(diffuseMap, fs_in.TexCoords).rgb;
    // ambient
    vec3 ambient = 0.1 * color;
    // diffuse
    vec3 lightDir = normalize(lightPos - fs_in.FragPos);
    float diff = max(dot(lightDir, normal), 0.0);
    vec3 diffuse = diff * color;
    // specular
    vec3 viewDir = normalize(viewPos - fs_in.FragPos);
    vec3 reflectDir = reflect(-lightDir, normal);
    vec3 halfwayDir = normalize(lightDir + viewDir);
    float spec = pow(max(dot(normal, halfwayDir), 0.0), 32.0);

    vec3 specular = vec3(0.2) * spec;
    FragColor = vec4(ambient + diffuse + specular, 1.0);
  }
#+end_src

** 世界坐标变换到切线空间中

第二种方法的效果同第一种，我看不出区别，但是可能像书中所说那样：

#+begin_quote
这是一个极佳的优化，因为顶点着色器通常比像素着色器运行的少。
#+end_quote

+ 顶点着色器目前代码：

#+begin_src glsl
  # version 330 core

  layout (location = 0) in vec3 aPos;
  layout (location = 1) in vec3 aNormal;
  layout (location = 2) in vec2 aTexCoords;
  layout (location = 3) in vec3 aTangent;
  layout (location = 4) in vec3 aBitangent;

  out VS_OUT {
    vec3 FragPos;
    vec2 TexCoords;
    vec3 TangentLightPos;
    vec3 TangentViewPos;
    vec3 TangentFragPos;
  } vs_out;

  uniform mat4 projection;
  uniform mat4 view;
  uniform mat4 model;

  uniform vec3 lightPos;
  uniform vec3 viewPos;

  void main() {
    vs_out.FragPos = vec3(model * vec4(aPos, 1.0));
    vs_out.TexCoords = aTexCoords;

    vec3 T = normalize(vec3(model * vec4(aTangent, 0.0)));
    vec3 B = normalize(vec3(model * vec4(aBitangent, 0.0)));
    vec3 N = normalize(vec3(model * vec4(aNormal, 0.0)));
    mat3 TBN = transpose(mat3(T, B, N));
    vs_out.TangentLightPos = TBN * lightPos;
    vs_out.TangentViewPos = TBN * viewPos;
    vs_out.TangentFragPos = TBN * vec3(model * vec4(aPos, 1.0));

    gl_Position = projection * view * model * vec4(aPos, 1.0);
  }
#+end_src

+ 片段着色器目前代码：

#+begin_src glsl
  # version 330 core

  out vec4 FragColor;

  in VS_OUT {
    vec3 FragPos;
    vec2 TexCoords;
    vec3 TangentLightPos;
    vec3 TangentViewPos;
    vec3 TangentFragPos;
  } fs_in;

  uniform sampler2D diffuseMap;
  uniform sampler2D normalMap;
  uniform vec3 lightPos;
  uniform vec3 viewPos;

  void main() {
    // obtain normal from normal map in range [0, 1]
    vec3 normal = texture(normalMap, fs_in.TexCoords).rgb;
    // transform normal vector to range [-1, 1];
    normal = normalize(normal * 2.0 - 1.0);

    // get diffuse color
    vec3 color = texture(diffuseMap, fs_in.TexCoords).rgb;
    // ambient
    vec3 ambient = 0.1 * color;
    // diffuse
    vec3 lightDir = normalize(fs_in.TangentLightPos - fs_in.TangentFragPos);
    float diff = max(dot(lightDir, normal), 0.0);
    vec3 diffuse = diff * color;
    // specular
    vec3 viewDir = normalize(fs_in.TangentViewPos - fs_in.TangentFragPos);
    vec3 reflectDir = reflect(-lightDir, normal);
    vec3 halfwayDir = normalize(lightDir + viewDir);
    float spec = pow(max(dot(normal, halfwayDir), 0.0), 32.0);

    vec3 specular = vec3(0.2) * spec;
    FragColor = vec4(ambient + diffuse + specular, 1.0);
  }
#+end_src

关于书中提到的：格拉姆-施密特正交化过程（ ~Gram-Schmidt process~ ），网页上的代码是这样的：

#+begin_src glsl
  vec3 T = normalize(vec3(model * vec4(tangent, 0.0)));
  vec3 N = normalize(vec3(model * vec4(normal, 0.0)));
  // re-orthogonalize T with respect to N
  T = normalize(T - dot(T, N) * N);
  // then retrieve perpendicular vector B with the cross product of T and N
  vec3 B = cross(T, N);

  mat3 TBN = mat3(T, B, N)
#+end_src

例子代码是这样的：

#+begin_src glsl
  mat3 normalMatrix = transpose(inverse(mat3(model)));
  vec3 T = normalize(normalMatrix * aTangent);
  vec3 N = normalize(normalMatrix * aNormal);
  T = normalize(T - dot(T, N) * N);
  vec3 B = cross(N, T);

  mat3 TBN = transpose(mat3(T, B, N));
#+end_src

看不太懂，这两段代码表达是同一个意思吗？

* <2024-10-18 周五> 高级光照.视差贴图

这里读的是英文原文，确实比翻译的好懂，这里完全是在上节法线贴图的基础上稍微加点修改完成的，重点是要理解法线空间。

** 视差贴图

#+ATTR_HTML: :width 60%
[[file:files/parallax_mapping_scaled_height.png]]

我理解的原理（参考上图）：

1. 视线从纹理平面（用黑粗实线表示）的 ~A~ 点接触采样，但是实际看到的其实是 ~B~ 点，所以要从 ~A~ 点采样 ~B~ 点纹理。
2. 先取样 ~A~ 点的高度（从位移图中），然后将观察方向上的向量缩放到这个长度，即 ~P~ 向量。
3. 该 ~P~ 向量在纹理平面上的位移值可以取到了。
4. 有了这个位移值就可以得到点处的纹理坐标，并能从位移图中取得它的高度值，即 ~H(P)~ ，它与 ~B~ 点非常接近。
5. 所以采样该点的纹理就相当于近似采样到了 ~B~ 点的纹理。这就是我理解的原理了。

我不太理解的是 ~P~ 向量的计算公式：

#+begin_src glsl
  vec2 p = viewDir.xy / viewDir.z * (height * height_scale);
#+end_src

最后浏览了一下 ~陡峭视差映射~ 和 ~视差遮蔽映射~ 原理也不难，全部都在片段着色器中进行。总的来说，重点还是上节的法线贴图，理解好它的切线空间，到这里的视差贴图就完全没有压力了。

** 陡峭视差映射（ ~Steep Parallax Mapping~ ）

正面看上去效果确实不错，但是侧面看即明显看出有分层的现象：

#+ATTR_HTML: :width 60%
[[file:files/20241020_0.png]]

#+ATTR_HTML: :width 60%
[[file:files/20241020_1.png]]

当前片段着色器代码如下：

#+begin_src glsl
  # version 330 core

  out vec4 FragColor;

  in VS_OUT {
    vec3 FragPos;
    vec2 TexCoords;
    vec3 TangentLightPos;
    vec3 TangentViewPos;
    vec3 TangentFragPos;
  } fs_in;

  uniform sampler2D diffuseMap;
  uniform sampler2D normalMap;
  uniform sampler2D depthMap;
  uniform float heightScale;

  vec2 ParallaxMapping(vec2 texCoords, vec3 viewDir) {
    // number of depth layers
    const float numLayers = 10;
    // calcuate the size of each layer
    float layerDepth = 1.0 / numLayers;
    // depth of current layer
    float currentLayerDepth = 0.0;
    // the amount to shift the texture coordinates per layer (from vector p)
    vec2 P = viewDir.xy * heightScale;
    vec2 deltaTexCoords = P / numLayers;
    // get initial values
    vec2 currentTexCoords = texCoords;
    float currentDepthMapValue = texture(depthMap, currentTexCoords).r;

    while (currentLayerDepth < currentDepthMapValue) {
      // shift texture coordinates along direction of P
      currentTexCoords -= deltaTexCoords;
      // get depthmap value at current texture coordinates
      currentDepthMapValue = texture(depthMap, currentTexCoords).r;
      // get depth of next layer
      currentLayerDepth += layerDepth;
    }

    return currentTexCoords;
  }

  void main() {
    // offset texture coordinates with Parallax Mapping
    vec3 viewDir = normalize(fs_in.TangentViewPos - fs_in.TangentFragPos);
    vec2 texCoords = ParallaxMapping(fs_in.TexCoords, viewDir);
    texCoords = ParallaxMapping(fs_in.TexCoords,  viewDir);
    if(texCoords.x > 1.0 || texCoords.y > 1.0 || texCoords.x < 0.0 || texCoords.y < 0.0)
      discard;

    // 接下来的所有采样使用 texCoords，而不是 fs_in.TexCoords
    // obtain normal from normal map in range [0, 1]
    vec3 normal = texture(normalMap, texCoords).rgb;
    // transform normal vector to range [-1, 1];
    normal = normalize(normal * 2.0 - 1.0);

    // get diffuse color
    vec3 color = texture(diffuseMap, texCoords).rgb;
    // ambient
    vec3 ambient = 0.1 * color;
    // diffuse
    vec3 lightDir = normalize(fs_in.TangentLightPos - fs_in.TangentFragPos);
    float diff = max(dot(lightDir, normal), 0.0);
    vec3 diffuse = diff * color;
    // specular
    vec3 reflectDir = reflect(-lightDir, normal);
    vec3 halfwayDir = normalize(lightDir + viewDir);
    float spec = pow(max(dot(normal, halfwayDir), 0.0), 32.0);

    vec3 specular = vec3(0.2) * spec;
    FragColor = vec4(ambient + diffuse + specular, 1.0);
  }
#+end_src

** 视差遮蔽映射（ ~Parallax Occlusion Mapping~ ）

#+ATTR_HTML: :width 60%
[[file:files/20241020_2.png]]

视差遮蔽映射的代码基于陡峭视差映射，所以并不难：

#+begin_src glsl
  vec2 ParallaxMapping(vec2 texCoords, vec3 viewDir) {
    // number of depth layers
    const float numLayers = 10;
    // calcuate the size of each layer
    float layerDepth = 1.0 / numLayers;
    // depth of current layer
    float currentLayerDepth = 0.0;
    // the amount to shift the texture coordinates per layer (from vector p)
    vec2 P = viewDir.xy * heightScale;
    vec2 deltaTexCoords = P / numLayers;
    // get initial values
    vec2 currentTexCoords = texCoords;
    float currentDepthMapValue = texture(depthMap, currentTexCoords).r;

    while (currentLayerDepth < currentDepthMapValue) {
      // shift texture coordinates along direction of P
      currentTexCoords -= deltaTexCoords;
      // get depthmap value at current texture coordinates
      currentDepthMapValue = texture(depthMap, currentTexCoords).r;
      // get depth of next layer
      currentLayerDepth += layerDepth;
    }

    // get texture coordinates before collision (reverse operations)
    vec2 prevTexCoords = currentTexCoords + deltaTexCoords;

    // get depth after and before collision for linear interpolation
    float afterDepth  = currentDepthMapValue - currentLayerDepth;
    float beforeDepth = texture(depthMap, prevTexCoords).r - currentLayerDepth + layerDepth;

    // interpolation of texture coordinates
    float weight = afterDepth / (afterDepth - beforeDepth);
    vec2 finalTexCoords = prevTexCoords * weight + currentTexCoords * (1.0 - weight);

    return finalTexCoords;
  }
#+end_src

* <2024-10-18 周五> 高级光照. ~HDR~

#+begin_quote
举例来说，高曝光值会使隧道的黑暗部分显示更多的细节，然而低曝光值会显著减少黑暗区域的细节，但允许我们看到更多明亮区域的细节。
#+end_quote

#+ATTR_HTML: :width 60%
[[file:files/hdr_exposure.png]]

莫非单反上的高曝光和低曝光也是这个意思？见上图：

1. 中间的图是 *高曝光* 可以看到黑暗处更多的细节。
2. 最右的图是 *低曝光* 可以看到明亮处更多的细节。
